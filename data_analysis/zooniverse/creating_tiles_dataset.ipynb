{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.5.3\n",
      "4.5.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(1, '../..')\n",
    "from utils import *\n",
    "import cv2\n",
    "import convml_tt\n",
    "from convml_tt.system import TripletTrainerModel, TripletTrainerDataModule\n",
    "from convml_tt.data.examples import fetch_example_dataset, ExampleData, fetch_pretrained_model, PretrainedModel\n",
    "from convml_tt.data.dataset import TileType, ImageSingletDataset\n",
    "from convml_tt.utils import get_embeddings\n",
    "import convml_tt.interpretation\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.collections import PatchCollection\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from PIL import Image\n",
    "import os, shutil\n",
    "from matplotlib.patches import Rectangle as rectan\n",
    "from random import seed\n",
    "from random import randint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from convml_tt.interpretation.rectpred.transform import apply_transform\n",
    "from convml_tt.data.examples import load_pretrained_model, PretrainedModel\n",
    "from convml_tt.interpretation.rectpred.data import make_sliding_tile_model_predictions\n",
    "from convml_tt.interpretation.rectpred.plot import make_rgb\n",
    "from tqdm.notebook import tqdm\n",
    "from math import sqrt, cos, sin\n",
    "import random\n",
    "import operator\n",
    "\n",
    "TILE_FILENAME_FORMAT = \"{triplet_id:05d}_{tile_type}.png\"\n",
    "TEST_SIZE = 0.3\n",
    "RANDOM_STATE = 1024\n",
    "COLORS = ['b', 'g', 'r', 'm'] # Color of each class\n",
    "#DATASETS_path = \"../../../DATASETS/\"\n",
    "#DATASET_DIR = \"../../../DATASETS/zooniverse_kaggle/\"\n",
    "DATASETS_path = \"/home/fbrient/Dropbox/Documents/EUREC4A/Data/\"\n",
    "DATASET_DIR = DATASETS_path+\"Kaggle/\"\n",
    "print(cv2.__version__)\n",
    "\n",
    "pca = PCA(n_components=3, svd_solver='arpack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fbrient/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "#df = pd.read_csv(\"../../../DATASETS/zooniverse_kaggle/train.csv\")\n",
    "df = pd.read_csv(DATASET_DIR+\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fbrient/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image_Label</th>\n",
       "      <th>EncodedPixels</th>\n",
       "      <th>Image</th>\n",
       "      <th>Class</th>\n",
       "      <th>Fish</th>\n",
       "      <th>Flower</th>\n",
       "      <th>Gravel</th>\n",
       "      <th>Sugar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0011165.jpg_Fish</td>\n",
       "      <td>264918 937 266318 937 267718 937 269118 937 27...</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Fish</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0011165.jpg_Flower</td>\n",
       "      <td>1355565 1002 1356965 1002 1358365 1002 1359765...</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Flower</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0011165.jpg_Gravel</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Gravel</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0011165.jpg_Sugar</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0011165.jpg</td>\n",
       "      <td>Sugar</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>002be4f.jpg_Fish</td>\n",
       "      <td>233813 878 235213 878 236613 878 238010 881 23...</td>\n",
       "      <td>002be4f.jpg</td>\n",
       "      <td>Fish</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Image_Label                                      EncodedPixels  \\\n",
       "0    0011165.jpg_Fish  264918 937 266318 937 267718 937 269118 937 27...   \n",
       "1  0011165.jpg_Flower  1355565 1002 1356965 1002 1358365 1002 1359765...   \n",
       "2  0011165.jpg_Gravel                                                NaN   \n",
       "3   0011165.jpg_Sugar                                                NaN   \n",
       "4    002be4f.jpg_Fish  233813 878 235213 878 236613 878 238010 881 23...   \n",
       "\n",
       "         Image   Class  Fish  Flower  Gravel  Sugar  \n",
       "0  0011165.jpg    Fish     1       0       0      0  \n",
       "1  0011165.jpg  Flower     0       1       0      0  \n",
       "2  0011165.jpg  Gravel     0       0       1      0  \n",
       "3  0011165.jpg   Sugar     0       0       0      1  \n",
       "4  002be4f.jpg    Fish     1       0       0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Image'] = df['Image_Label'].map(lambda x: x.split('_')[0])\n",
    "df['Class'] = df['Image_Label'].map(lambda x: x.split('_')[1])\n",
    "classes = df['Class'].unique()\n",
    "train_df = df.groupby('Image')['Class'].agg(set).reset_index()\n",
    "for class_name in classes:\n",
    "    df[class_name] = df['Class'].map(lambda x: 1 if class_name in x else 0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fbrient/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "# Creating training samples\n",
    "\n",
    "image_col = np.array(df['Image'])\n",
    "image_files = image_col[::4]\n",
    "all_labels = df['Class']\n",
    "X_train, y_train = image_files, all_labels\n",
    "train_pairs = np.array(list(zip(X_train, y_train)))\n",
    "TILE_FILENAME_FORMAT = \"{triplet_id:05d}_{tile_type}.png\"\n",
    "NUM_TRAIN_SAMPLES = len(train_pairs)\n",
    "train_samples = train_pairs[np.random.choice(train_pairs.shape[0], NUM_TRAIN_SAMPLES, replace=False), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset of rectangles divided in tiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fbrient/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24a559b72b074a51805cda7d46d2d307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "labs = []\n",
    "shown = True\n",
    "t = 0\n",
    "pathout = DATASETS_path+ 'ZOONIVERSE_DIVIDED/'\n",
    "for sample in tqdm(train_samples):\n",
    "        if not shown:\n",
    "            fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        img_path = os.path.join(DATASET_DIR, 'train_images', sample[0])\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #plt.imshow(img)\n",
    "        #plt.show()\n",
    "        # Get annotations\n",
    "        labels = df[df['Image_Label'].str.contains(sample[0])]['EncodedPixels']\n",
    "\n",
    "        patches = []\n",
    "        for idx, rle in enumerate(labels.values):\n",
    "            if rle is not np.nan:\n",
    "                mask = rle2mask(rle)\n",
    "                #maskn = mask.copy()\n",
    "                #mask = np.ascontiguousarray(mask , dtype=np.uint8)\n",
    "                contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                #print(\"Contours : \"+str(contours))\n",
    "                rgbImg = extract_contour(img, contours, shown=shown)\n",
    "                rgbImg = cv2.cvtColor(rgbImg, cv2.COLOR_BGR2RGB)\n",
    "                for x in range(0, rgbImg.shape[0], 256):\n",
    "                    for y in range(0, rgbImg.shape[1], 256):\n",
    "                        if rgbImg[x:x+256,y:y+256].shape != (256, 256, 3) or [0,0,0] in rgbImg[x:x+256,y:y+256]:\n",
    "                            continue\n",
    "                        \n",
    "                        tm = t\n",
    "                        out_name = TILE_FILENAME_FORMAT.format(triplet_id=tm,tile_type='anchor')\n",
    "                        \n",
    "                        out_name = pathout+'train/'+out_name\n",
    "                        cv2.imwrite(out_name,rgbImg[x:x+256,y:y+256])\n",
    "                        t += 1\n",
    "                        labs.append(idx)\n",
    "                if not shown:\n",
    "                    for contour in contours:\n",
    "                        poly_patch = Polygon(contour.reshape(-1, 2), closed=True, linewidth=2, edgecolor=COLORS[idx], facecolor=COLORS[idx], fill=True)\n",
    "                        patches.append(poly_patch)\n",
    "        if not shown:\n",
    "            shown = True\n",
    "            p = PatchCollection(patches, match_original=True, cmap=matplotlib.cm.jet, alpha=0.3)\n",
    "\n",
    "            ax.imshow(img/255)\n",
    "            ax.set_title('{} - ({})'.format(sample[0], ', '.join(sample[1].astype(np.str))))\n",
    "            ax.add_collection(p)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            plt.show()\n",
    "with open(pathout+'labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(labs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating division with rectangles resized as images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fbrient/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4193460919934731876696e1aae9eb3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "labs_resized = []\n",
    "shown = False\n",
    "t = 0\n",
    "pathout = DATASETS_path+ 'ZOONIVERSE_DIVIDED/'\n",
    "\n",
    "for sample in tqdm(train_samples):\n",
    "        img_path = os.path.join(DATASET_DIR, 'train_images', sample[0])\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        labels = df[df['Image_Label'].str.contains(sample[0])]['EncodedPixels']\n",
    "\n",
    "        patches = []\n",
    "        for idx, rle in enumerate(labels.values):\n",
    "            if rle is not np.nan:\n",
    "                mask = rle2mask(rle)\n",
    "                contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                rgbImg = extract_contour(img, contours, shown=False)\n",
    "                rgbImg = cv2.resize(rgbImg, (256, 256), interpolation = cv2.INTER_AREA)\n",
    "                rgbImg = cv2.cvtColor(rgbImg, cv2.COLOR_BGR2RGB)\n",
    "                labs_resized.append(idx)\n",
    "                tm = t\n",
    "                out_name = TILE_FILENAME_FORMAT.format(triplet_id=tm,tile_type='anchor')                \n",
    "                out_name = pathout+'train/'+out_name\n",
    "                cv2.imwrite(out_name, rgbImg)\n",
    "                t += 1\n",
    "with open(pathout+'labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(labs_resized, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating training dataset for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fbrient/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4797e0be449479f8cf4f000924ad07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-551d1b66284d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mdistant_img_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_images'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mdistant_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistant_img_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0;31m#distant_img = cv2.cvtColor(distant_img, cv2.COLOR_BGR2RGB)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mx_distant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistant_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m257\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 607\n",
    "train_saving_path = DATASETS_path+'ZOONIVERSE_TRAIN/train/'\n",
    "study_saving_path = DATASETS_path+'ZOONIVERSE_TRAIN/study/'\n",
    "saving_path = train_saving_path\n",
    "PI = 3.1456\n",
    "t = 0\n",
    "index_img = 0\n",
    "for sample in tqdm(train_samples):\n",
    "    if index_img < 0.81*train_samples.shape[0]:\n",
    "        saving_path = train_saving_path\n",
    "    else:\n",
    "        saving_path = study_saving_path\n",
    "    img_path = os.path.join(DATASET_DIR, 'train_images', sample[0])\n",
    "    img = cv2.imread(img_path, 1)\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Get annotations\n",
    "    labels = df[df['Image_Label'].str.contains(sample[0])]['EncodedPixels']\n",
    "\n",
    "    for x in range(0, img.shape[0], 256):\n",
    "        for y in range(0, img.shape[1], 256):\n",
    "            if img[x:x+256,y:y+256].shape != (256, 256, 3):# or  [0,0,0] in rgbImg[x:x+256,y:y+256]:\n",
    "                continue\n",
    "            start = time.time()\n",
    "            time.perf_counter()            \n",
    "                \n",
    "            found = False # Variable meaning that we didn't find suiting neighobr tile\n",
    "            best_neighbor = 0\n",
    "            best_neighbor_img = []\n",
    "            while not found: # Getting a neighbor image\n",
    "                R = 256/2\n",
    "                theta = random.random() * 2 * PI\n",
    "\n",
    "                \n",
    "                x_shift, y_shift = int(R * cos(theta)), int(R * sin(theta))\n",
    "                neighobr_img = img[x+x_shift:x+x_shift+256,y+y_shift:y+y_shift+256]\n",
    "                elapsed = time.time() - start\n",
    "                if len(np.unique(neighobr_img.reshape(-1, neighobr_img.shape[2]), axis=0))>best_neighbor and neighobr_img.shape == (256, 256, 3):\n",
    "                    best_neighbor = len(np.unique(neighobr_img.reshape(-1, neighobr_img.shape[2]), axis=0))\n",
    "                    best_neighbor_img = neighobr_img\n",
    "\n",
    "                if (not (neighobr_img.shape != (256, 256, 3) or len(np.unique(neighobr_img.reshape(-1, neighobr_img.shape[2]), axis=0))<30)) or elapsed>30:\n",
    "                    found = True\n",
    "            if [0, 0, 0] in best_neighbor_img:\n",
    "                continue\n",
    "            neighobr_img = best_neighbor_img\n",
    "            found = False # distant\n",
    "\n",
    "            while not found:\n",
    "                random_img = train_samples[random.randint(0, train_samples.shape[0]-1)]\n",
    "                if random_img[0] == sample[0]:\n",
    "                    continue\n",
    "                distant_img_path = os.path.join(DATASET_DIR, 'train_images', random_img[0])\n",
    "                distant_img = cv2.imread(distant_img_path, 1)\n",
    "                #distant_img = cv2.cvtColor(distant_img, cv2.COLOR_BGR2RGB)\n",
    "                x_distant = random.randint(0, distant_img.shape[0]-257)\n",
    "                y_distant = random.randint(0, distant_img.shape[1]-257)\n",
    "                distant_img_shaped = distant_img[x_distant:x_distant+256,y_distant:y_distant+256]\n",
    "                if not (distant_img_shaped.shape != (256, 256, 3) or [0,0,0] in distant_img_shaped):\n",
    "                    found = True\n",
    "                \n",
    "            tm = t\n",
    "            \n",
    "            # anchor\n",
    "            out_name = TILE_FILENAME_FORMAT.format(triplet_id=tm,tile_type='anchor')\n",
    "            out_name = saving_path + out_name\n",
    "            cv2.imwrite(out_name, img[x:x+256,y:y+256])\n",
    "\n",
    "            # neighbor\n",
    "            out_name_neighbor = TILE_FILENAME_FORMAT.format(triplet_id=tm,tile_type='neighbor')\n",
    "            out_name_neighbor = saving_path + out_name_neighbor\n",
    "            cv2.imwrite(out_name_neighbor, neighobr_img)\n",
    "\n",
    "            # distant\n",
    "            out_name_distant = TILE_FILENAME_FORMAT.format(triplet_id=tm,tile_type='distant')\n",
    "            out_name_distant = saving_path + out_name_distant\n",
    "            cv2.imwrite(out_name_distant, distant_img_shaped)\n",
    "\n",
    "            t += 1\n",
    "    index_img += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fbrient/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import kornia\n",
    "print(kornia.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fbrient/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating dataset threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fbrient/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c0ea041ef34db6877c33fbd5a05eba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5546 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "labs = []\n",
    "shown = True\n",
    "t = 0\n",
    "stats = {}\n",
    "for sample in tqdm(train_samples):\n",
    "        if not shown:\n",
    "            fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        img_path = os.path.join(DATASET_DIR, 'train_images', sample[0])\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #plt.imshow(img)\n",
    "        #plt.show()\n",
    "        # Get annotations\n",
    "        labels = df[df['Image_Label'].str.contains(sample[0])]['EncodedPixels']\n",
    "        #print(\"labels : \"+str(labels.values))\n",
    "        patches = []\n",
    "        dominance = 0\n",
    "        recapitulatif = {}\n",
    "        all_size = (0, 0, 0)\n",
    "        for idx, rle in enumerate(labels.values):\n",
    "            if rle is not np.nan:\n",
    "                mask = rle2mask(rle)\n",
    "                #maskn = mask.copy()\n",
    "                #mask = np.ascontiguousarray(mask , dtype=np.uint8)\n",
    "                contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                #print(\"Contours : \"+str(contours))\n",
    "                rgbImages = extract_contour(img, contours, shown=shown, direct=False)\n",
    "                \"\"\"\n",
    "                print(\"IDX : \"+str(idx))\n",
    "                print(\"len : \"+str(len(rgbImages)))\n",
    "                print(\"first : \"+str(rgbImages[0].shape))\n",
    "                \"\"\"\n",
    "                summed = (0, 0)\n",
    "                for rgbImg in rgbImages:\n",
    "                    summed = tuple(map(operator.add, summed, rgbImg.shape))\n",
    "                    #print(type(rgbImg.shape))\n",
    "                #print(\"mean of class : \"+str(summed)+\"\\n\")\n",
    "                all_size = (all_size[0]+summed[0], all_size[1]+summed[1]) #tuple(map(operator.add, all_size, summed))\n",
    "                recapitulatif[str(idx)] = (summed[0],summed[1])\n",
    "                               \n",
    "                \"\"\"\n",
    "                for rgbImg in rgbImages:\n",
    "                    print(rgbImg.shape)\n",
    "                \"\"\"\n",
    "                #break\n",
    "                #rgbImg = cv2.cvtColor(rgbImg, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "                for x in range(0, rgbImg.shape[0], 256):\n",
    "                    for y in range(0, rgbImg.shape[1], 256):\n",
    "                        if rgbImg[x:x+256,y:y+256].shape != (256, 256, 3) or [0,0,0] in rgbImg[x:x+256,y:y+256]:\n",
    "                            continue\n",
    "                        \n",
    "                        tm = t\n",
    "                        out_name = TILE_FILENAME_FORMAT.format(triplet_id=tm,tile_type='anchor')\n",
    "                        \n",
    "                        out_name = '../NC/division/train/'+out_name\n",
    "                        cv2.imwrite(out_name,rgbImg[x:x+256,y:y+256])\n",
    "                        t += 1\n",
    "                        labs.append(idx)\n",
    "                if not shown:\n",
    "                    for contour in contours:\n",
    "                        #print(\"contour : \"+str(contour))\n",
    "                        poly_patch = Polygon(contour.reshape(-1, 2), closed=True, linewidth=2, edgecolor=COLORS[idx], facecolor=COLORS[idx], fill=True)\n",
    "                        patches.append(poly_patch)\n",
    "                \n",
    "        if not shown:\n",
    "            shown = True\n",
    "            p = PatchCollection(patches, match_original=True, cmap=matplotlib.cm.jet, alpha=0.3)\n",
    "\n",
    "            ax.imshow(img/255)\n",
    "            ax.set_title('{} - ({})'.format(sample[0], ', '.join(sample[1].astype(np.str))))\n",
    "            ax.add_collection(p)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            plt.show()\n",
    "        \n",
    "        \"\"\"\n",
    "        print(\"\\n\\nlast all size : \"+str(all_size))\n",
    "        #all_size = all_size[0]*all_size[1]\n",
    "        \"\"\"\n",
    "        recapitulatif_percentage = {}\n",
    "        #print(\"RECAPITULATIF X Y : \"+str(recapitulatif))\n",
    "        for key in recapitulatif:\n",
    "            recapitulatif_percentage[key] = (recapitulatif[key][0]/all_size[0] + recapitulatif[key][1]/all_size[1])/2\n",
    "        best = 0\n",
    "        best_idx = -1\n",
    "        for key in recapitulatif_percentage:\n",
    "            if recapitulatif_percentage[key]>best:\n",
    "                best = recapitulatif_percentage[key]\n",
    "        try:\n",
    "            stats[round(best, 2)] += 1\n",
    "        except:\n",
    "            stats[round(best, 2)] = 1\n",
    "        \"\"\"\n",
    "        print(\"\\nRECAPITULATIF : \"+str(recapitulatif))\n",
    "        print(\"RECAPITULATIF PERCENTAGE : \"+str(recapitulatif_percentage))\n",
    "        print(\"SUMMED : \"+str(all_size))\n",
    "        break\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(35, 10)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig, ax = plt.subplots()\n",
    "keys = list(stats.keys())\n",
    "# get values in the same order as keys, and parse percentage values\n",
    "#vals = [float(stats[k][:-1]) for k in keys]\n",
    "sns.barplot(x=keys, y=list(stats.values()), ax=ax)\n",
    "ax.set_ylabel('Number of images',fontsize=40)\n",
    "ax.set_xlabel('Best Coverage Percentage',fontsize=40)\n",
    "#ax.xticks(fontsize=14, rotation=90)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.title(\"Plot of coverage percentage Zooniverse Dataset\")\n",
    "plt.savefig('coverage.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "stacked = 0\n",
    "stacked_stats = {}\n",
    "stats = collections.OrderedDict(sorted(stats.items()))\n",
    "for key in stats:\n",
    "    stacked += stats[key]\n",
    "    stacked_stats[key] = stacked\n",
    "    #print(stacked)\n",
    "print(stacked_stats)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.set(rc={'figure.figsize':(35, 40)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "keys_stacked = list(stacked_stats.keys())\n",
    "# get values in the same order as keys, and parse percentage values\n",
    "#vals = [float(stats[k][:-1]) for k in keys]\n",
    "sns.barplot(x=keys_stacked, y=list(stacked_stats.values()), ax=ax)\n",
    "ax.set_ylabel('Number of images',fontsize=40)\n",
    "ax.set_xlabel('Best Coverage Percentage',fontsize=40)\n",
    "#ax.xticks(fontsize=14, rotation=90)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.title(\"Cumulative plot of coverage percentage Zooniverse Dataset\")\n",
    "plt.savefig('cumulative_coverage.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution couverture des classes / shape image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "labs = []\n",
    "shown = True\n",
    "t = 0\n",
    "stats = {}\n",
    "index_distributions = {\n",
    "    '0':{},\n",
    "    '1':{},\n",
    "    '2':{},\n",
    "    '3':{},\n",
    "}\n",
    "for i in range(0, 101):\n",
    "    index_distributions['0'][str(i/100)] = 0\n",
    "    index_distributions['1'][str(i/100)] = 0\n",
    "    index_distributions['2'][str(i/100)] = 0\n",
    "    index_distributions['3'][str(i/100)] = 0\n",
    "\n",
    "for sample in tqdm(train_samples):\n",
    "        if not shown:\n",
    "            fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        img_path = os.path.join(DATASET_DIR, 'train_images', sample[0])\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #plt.imshow(img)\n",
    "        #plt.show()\n",
    "        # Get annotations\n",
    "        labels = df[df['Image_Label'].str.contains(sample[0])]['EncodedPixels']\n",
    "        #print(\"labels : \"+str(labels.values))\n",
    "        patches = []\n",
    "        dominance = 0\n",
    "        recapitulatif = {}\n",
    "        all_size = (0, 0, 0)\n",
    "        for idx, rle in enumerate(labels.values):\n",
    "            if rle is not np.nan:\n",
    "                mask = rle2mask(rle)\n",
    "                contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                #print(\"Contours : \"+str(contours))\n",
    "                \n",
    "                img_copy = np.zeros(img.shape)\n",
    "                cnts = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "                rois = []\n",
    "                # Find bounding box and extract ROI\n",
    "                for c in cnts:\n",
    "                    x,y,w,h = cv2.boundingRect(c)\n",
    "                    img_copy[y:y+h, x:x+w] = (1, 1, 1)\n",
    "                percentage = np.count_nonzero((img_copy==(1, 1, 1)).all(axis = 2)) / ( img_copy.shape[0]*img_copy.shape[1] )\n",
    "                percentage_round = round(percentage, 2)\n",
    "                index_distributions[str(idx)][str(percentage_round)] += 1\n",
    "                \"\"\"\n",
    "                print(\"sample : \"+str(img_path))\n",
    "                print(\"idx : \"+str(idx))\n",
    "                print(\"% pixels \"+str(  ) )\n",
    "                \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations = ['Fish', 'Flower', 'Gravel', 'Sugar']\n",
    "sns.set(rc={'figure.figsize':(60, 20)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "for classe in index_distributions:\n",
    "    fig, ax = plt.subplots()    \n",
    "    keys_stacked = list(index_distributions[classe].keys())\n",
    "    # get values in the same order as keys, and parse percentage values\n",
    "    #vals = [float(stats[k][:-1]) for k in keys]\n",
    "    sns.barplot(x=keys_stacked, y=list(index_distributions[classe].values()), ax=ax)\n",
    "    ax.set_ylabel('Number of images',fontsize=40)\n",
    "    ax.set_xlabel('Coverage Percentage',fontsize=40)\n",
    "    #ax.xticks(fontsize=14, rotation=90)\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.figtext(.5,.9,str(associations[int(classe)])+\" Coverage % on Images\", fontsize=100, ha='center')\n",
    "    plt.savefig(str(associations[int(classe)])+'_coverage.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting average shape of rectangles for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "labs = []\n",
    "shown = True\n",
    "t = 0\n",
    "stats = {}\n",
    "index_distributions = {\n",
    "    '0':{'x':{}, 'y':{}},\n",
    "    '1':{'x':{}, 'y':{}},\n",
    "    '2':{'x':{}, 'y':{}},\n",
    "    '3':{'x':{}, 'y':{}},\n",
    "}\n",
    "\n",
    "for i in range(0, 2101):\n",
    "    index_distributions['0']['x'][str(i)] = 0\n",
    "    index_distributions['0']['y'][str(i)] = 0\n",
    "\n",
    "    index_distributions['1']['x'][str(i)] = 0\n",
    "    index_distributions['1']['y'][str(i)] = 0\n",
    "\n",
    "    index_distributions['2']['x'][str(i)] = 0\n",
    "    index_distributions['2']['y'][str(i)] = 0\n",
    "\n",
    "    index_distributions['3']['x'][str(i)] = 0\n",
    "    index_distributions['3']['y'][str(i)] = 0\n",
    "\n",
    "for sample in tqdm(train_samples):\n",
    "        if not shown:\n",
    "            fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        img_path = os.path.join(DATASET_DIR, 'train_images', sample[0])\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        #plt.imshow(img)\n",
    "        #plt.show()\n",
    "        # Get annotations\n",
    "        labels = df[df['Image_Label'].str.contains(sample[0])]['EncodedPixels']\n",
    "        #print(\"labels : \"+str(labels.values))\n",
    "        patches = []\n",
    "        dominance = 0\n",
    "        recapitulatif = {}\n",
    "        all_size = (0, 0, 0)\n",
    "        for idx, rle in enumerate(labels.values):\n",
    "            if rle is not np.nan:\n",
    "                mask = rle2mask(rle)\n",
    "                contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                #print(\"Contours : \"+str(contours))\n",
    "                \n",
    "                img_copy = np.zeros(img.shape)\n",
    "                cnts = sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "                rois = []\n",
    "                # Find bounding box and extract ROI\n",
    "                for c in cnts:\n",
    "                    x,y,w,h = cv2.boundingRect(c)\n",
    "                    img_copy[y:y+h, x:x+w] = (1, 1, 1)\n",
    "                    index_distributions[str(idx)]['x'][str(h)] += 1\n",
    "                    index_distributions[str(idx)]['y'][str(w)] += 1\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(index_distributions['1']['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations = ['Fish', 'Flower', 'Gravel', 'Sugar']\n",
    "sns.set(rc={'figure.figsize':(600, 20)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "for classe in index_distributions:\n",
    "    fig, ax = plt.subplots()    \n",
    "    keys_stacked = list(index_distributions[classe]['x'].keys())\n",
    "    # get values in the same order as keys, and parse percentage values\n",
    "    #vals = [float(stats[k][:-1]) for k in keys]\n",
    "    sns.barplot(x=keys_stacked, y=list(index_distributions[classe]['x'].values()), ax=ax)\n",
    "    ax.set_ylabel('Number of images',fontsize=40)\n",
    "    ax.set_xlabel('Coverage Percentage',fontsize=40)\n",
    "    #ax.xticks(fontsize=14, rotation=90)\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.figtext(.5,.9,str(associations[int(classe)])+\" Coverage % on Images\", fontsize=100, ha='center')\n",
    "    plt.savefig(str(associations[int(classe)])+'_size_shape_x_coverage.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "associations = ['Fish', 'Flower', 'Gravel', 'Sugar']\n",
    "sns.set(rc={'figure.figsize':(600, 20)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "for classe in index_distributions:\n",
    "    fig, ax = plt.subplots()    \n",
    "    keys_stacked = list(index_distributions[classe]['y'].keys())\n",
    "    # get values in the same order as keys, and parse percentage values\n",
    "    #vals = [float(stats[k][:-1]) for k in keys]\n",
    "    sns.barplot(x=keys_stacked, y=list(index_distributions[classe]['y'].values()), ax=ax)\n",
    "    ax.set_ylabel('Number of images',fontsize=40)\n",
    "    ax.set_xlabel('Coverage Percentage',fontsize=40)\n",
    "    #ax.xticks(fontsize=14, rotation=90)\n",
    "    plt.yticks(fontsize=30)\n",
    "    plt.figtext(.5,.9,str(associations[int(classe)])+\" Coverage % on Images\", fontsize=100, ha='center')\n",
    "    plt.savefig(str(associations[int(classe)])+'_size_shape_y_coverage.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation du jeu de données pour l'entrainement de la classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = []\n",
    "shown = True\n",
    "t = 0\n",
    "associations = ['Fish', 'Flower', 'Gravel', 'Sugar']\n",
    "PATH_TRAIN = DATASETS_path+\"CLASSIF_ALL_IMAGE/\"\n",
    "count_class = {}; count_class[\"Fish\"] = 0; count_class[\"Flower\"] = 0; count_class[\"Gravel\"] = 0; count_class[\"Sugar\"] = 0;\n",
    "stats = {}\n",
    "for sample in tqdm(train_samples):\n",
    "        if not shown:\n",
    "            fig, ax = plt.subplots(figsize=(15, 10))\n",
    "        img_path = os.path.join(DATASET_DIR, 'train_images', sample[0])\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        labels = df[df['Image_Label'].str.contains(sample[0])]['EncodedPixels']\n",
    "        patches = []\n",
    "        dominance = 0\n",
    "        recapitulatif = {}\n",
    "        all_size = (0, 0, 0)\n",
    "        for idx, rle in enumerate(labels.values):\n",
    "            if rle is not np.nan:\n",
    "                mask = rle2mask(rle)\n",
    "                contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                rgbImages = extract_contour(img, contours, shown=shown, direct=False)\n",
    "                summed = (0, 0)\n",
    "                for rgbImg in rgbImages:\n",
    "                    summed = tuple(map(operator.add, summed, rgbImg.shape))\n",
    "                all_size = (all_size[0]+summed[0], all_size[1]+summed[1])\n",
    "                recapitulatif[str(idx)] = (summed[0],summed[1])\n",
    "                               \n",
    "            recapitulatif_percentage = {}\n",
    "        for key in recapitulatif:\n",
    "            recapitulatif_percentage[key] = (recapitulatif[key][0]/all_size[0] + recapitulatif[key][1]/all_size[1])/2\n",
    "        best = 0\n",
    "        best_idx = -1\n",
    "        for key in recapitulatif_percentage:\n",
    "            if recapitulatif_percentage[key]>best:\n",
    "                best = recapitulatif_percentage[key]\n",
    "                best_idx = key\n",
    "        if best>0.64:\n",
    "            out_name = PATH_TRAIN+\"/\"+associations[int(best_idx)]+\"/\"+str(count_class[str(associations[int(best_idx)])])+\".jpg\"\n",
    "            #print(\"out name : \"+str(out_name))                      \n",
    "            cv2.imwrite(out_name, img)\n",
    "            count_class[str(associations[int(best_idx)])] += 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resized classidying dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = []\n",
    "shown = True\n",
    "t = 0\n",
    "associations = ['Fish', 'Flower', 'Gravel', 'Sugar']\n",
    "PATH_TRAIN = DATASETS_path+\"CLASSIF_RESIZED/\"\n",
    "count_class = {}; count_class[\"Fish\"] = 0; count_class[\"Flower\"] = 0; count_class[\"Gravel\"] = 0; count_class[\"Sugar\"] = 0;\n",
    "stats = {}\n",
    "for sample in tqdm(train_samples):\n",
    "        img_path = os.path.join(DATASET_DIR, 'train_images', sample[0])\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        labels = df[df['Image_Label'].str.contains(sample[0])]['EncodedPixels']\n",
    "        for idx, rle in enumerate(labels.values):\n",
    "            if rle is not np.nan:\n",
    "                mask = rle2mask(rle)\n",
    "                contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                rgbImg = extract_contour(img, contours, shown=shown, direct=True)\n",
    "\n",
    "                rgbImg = cv2.resize(rgbImg, (256, 256), interpolation = cv2.INTER_AREA)\n",
    "                #rgbImg = cv2.cvtColor(rgbImg, cv2.COLOR_BGR2RGB)              \n",
    "                out_name = PATH_TRAIN+str(associations[int(idx)])+\"/\"+str(count_class[associations[int(idx)]])+\".jpeg\"\n",
    "                cv2.imwrite(out_name, rgbImg)\n",
    "                count_class[associations[int(idx)]] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biggest square classifying dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs = []\n",
    "shown = True\n",
    "t = 0\n",
    "associations = ['Fish', 'Flower', 'Gravel', 'Sugar','Undefined']\n",
    "PATH_TRAIN = DATASETS_path+\"CLASSIF_BIGGEST_SQUARE/\"\n",
    "count_class = {}; count_class[\"Fish\"] = 0; count_class[\"Flower\"] = 0; count_class[\"Gravel\"] = 0; count_class[\"Sugar\"] = 0;\n",
    "count_class[\"Undefined\"] = 0\n",
    "stats = {}\n",
    "for sample in tqdm(train_samples):\n",
    "        img_path = os.path.join(DATASET_DIR, 'train_images', sample[0])\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        labels = df[df['Image_Label'].str.contains(sample[0])]['EncodedPixels']\n",
    "        cant = []\n",
    "        for idx, rle in enumerate(labels.values):\n",
    "            if rle is not np.nan:\n",
    "                mask = rle2mask(rle)\n",
    "                contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                cant.append(contours)\n",
    "                rgbImg = extract_contour(img, contours, shown=shown, direct=True)\n",
    "                min_shape = min(rgbImg.shape[0], rgbImg.shape[1])\n",
    "                if rgbImg.shape[0]>=128:\n",
    "                    rgbImg = cv2.resize(rgbImg[0:min_shape, 0:min_shape], (256, 256), interpolation = cv2.INTER_AREA)\n",
    "                    labs.append(idx)\n",
    "                    #rgbImg = cv2.cvtColor(rgbImg, cv2.COLOR_BGR2RGB)              \n",
    "                    out_name = PATH_TRAIN+str(associations[int(idx)])+\"/\"+str(count_class[associations[int(idx)]])+\".jpeg\"\n",
    "                    cv2.imwrite(out_name, rgbImg)\n",
    "                    count_class[associations[int(idx)]] += 1\n",
    "        for contours in cant:\n",
    "            for c in contours:\n",
    "                img = cv2.drawContours(img, [c], -1, (0,0,0), -1)\n",
    "\n",
    "        undef = extract_non_black(img)\n",
    "        undef = cv2.resize(undef, (256, 256), interpolation = cv2.INTER_AREA)\n",
    "        if [0,0,0] not in undef and undef.shape[0]>=128:\n",
    "            idx = 4\n",
    "            labs.append(idx)\n",
    "            #rgbImg = cv2.cvtColor(rgbImg, cv2.COLOR_BGR2RGB)              \n",
    "            out_name = PATH_TRAIN+str(associations[int(idx)])+\"/\"+str(count_class[associations[int(idx)]])+\".jpeg\"\n",
    "            cv2.imwrite(out_name, undef)\n",
    "            count_class[associations[int(idx)]] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(PATH_TRAIN+'labels.pickle', 'wb') as handle:\n",
    "    pickle.dump(labs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biggest Square clustering dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labs = []\n",
    "shown = False\n",
    "t = 0\n",
    "patches = []\n",
    "associations = ['Fish', 'Flower', 'Gravel', 'Sugar','Undefined']\n",
    "PATH_TRAIN = DATASETS_path+\"CLUSTERING_BIGGEST_SQUARE/train/\"\n",
    "count_class = {}; count_class[\"Fish\"] = 0; count_class[\"Flower\"] = 0; count_class[\"Gravel\"] = 0; count_class[\"Sugar\"] = 0; \n",
    "count_class[\"Undefined\"] = 0\n",
    "stats = {}\n",
    "a = 0\n",
    "print(\"LEN : \"+str(len(train_samples)))\n",
    "for sample in tqdm(train_samples):\n",
    "        img_path = os.path.join(DATASET_DIR, 'train_images', sample[0])\n",
    "        img = cv2.imread(img_path, 1)\n",
    "        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n",
    "        labels = df[df['Image_Label'].str.contains(sample[0])]['EncodedPixels']\n",
    "        patches = []\n",
    "        for idx, rle in enumerate(labels.values):\n",
    "            if rle is not np.nan:\n",
    "                mask = rle2mask(rle)\n",
    "                contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                rgbImg = extract_contour(img, contours, shown=shown, direct=True)\n",
    "                min_shape = min(rgbImg.shape[0], rgbImg.shape[1])\n",
    "                \n",
    "                \n",
    "                rgbImg = cv2.resize(rgbImg[0:min_shape, 0:min_shape], (256, 256), interpolation = cv2.INTER_AREA)\n",
    "                if [0,0,0] not in rgbImg and min_shape>=128:\n",
    "                    labs.append(idx)\n",
    "                                \n",
    "                    tm = t\n",
    "                    out_name = TILE_FILENAME_FORMAT.format(triplet_id=tm,tile_type='anchor')\n",
    "                    \n",
    "                    out_name = PATH_TRAIN+out_name\n",
    "\n",
    "                    cv2.imwrite(out_name, rgbImg)\n",
    "                    count_class[associations[int(idx)]] += 1\n",
    "                    t += 1\n",
    "            \n",
    "            for c in contours:\n",
    "                img = cv2.drawContours(img, [c], -1, (0,0,0), -1)\n",
    "\n",
    "        undef = extract_non_black(img)\n",
    "        undef = cv2.resize(undef, (256, 256), interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        if [0,0,0] not in undef and undef.shape[0]>=128:\n",
    "            tm = t\n",
    "            out_name = TILE_FILENAME_FORMAT.format(triplet_id=tm,tile_type='anchor')\n",
    "            \"\"\"\n",
    "            fig = plt.figure()\n",
    "            plt.imshow(undef)\n",
    "            fig.suptitle(out_name, fontsize=20)\n",
    "            plt.show()\n",
    "            \"\"\"\n",
    "            labs.append(4)\n",
    "            out_name = PATH_TRAIN+out_name\n",
    "            cv2.imwrite(out_name, undef)\n",
    "            count_class[associations[int(4)]] += 1\n",
    "            t += 1\n",
    "\n",
    "with open(DATASETS_path+\"CLUSTERING_BIGGEST_SQUARE/labels.pickle\", 'wb') as handle:\n",
    "    pickle.dump(labs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Scale generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#divisions = []\n",
    "from pathlib import Path\n",
    "import pathlib\n",
    "divisions = [64, 128, 256, 512, 612, 850, 1024 ] #range(416, 1024, 32)#\n",
    "divisions = [612]\n",
    "shown = False\n",
    "for div in divisions:\n",
    "    print(\"DIV : \"+str(div))\n",
    "    labs = []\n",
    "    shown = True\n",
    "    t = 0\n",
    "    associations = ['Fish', 'Flower', 'Gravel', 'Sugar']\n",
    "    PATH_TRAIN = DATASETS_path+\"MULTI_SCALE/\"+str(div)+\"/train/\"\n",
    "    if not Path(PATH_TRAIN).is_dir():\n",
    "        p = pathlib.Path(PATH_TRAIN)\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "    count_class = {}; count_class[\"Fish\"] = 0; count_class[\"Flower\"] = 0; count_class[\"Gravel\"] = 0; count_class[\"Sugar\"] = 0;\n",
    "    stats = {}\n",
    "    for sample in tqdm(train_samples):\n",
    "            img_path = os.path.join(DATASET_DIR, 'train_images', sample[0])\n",
    "            img = cv2.imread(img_path, 1)\n",
    "            labels = df[df['Image_Label'].str.contains(sample[0])]['EncodedPixels']\n",
    "            for idx, rle in enumerate(labels.values):\n",
    "                if rle is not np.nan:\n",
    "                    mask = rle2mask(rle)\n",
    "                    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                    rgbImg = extract_contour(img, contours, shown=shown, direct=True)\n",
    "\n",
    "                    for x in range(0, rgbImg.shape[0], div):\n",
    "                        for y in range(0, rgbImg.shape[1], div):\n",
    "                            if rgbImg[x:x+div,y:y+div].shape != (div, div, 3) or [0,0,0] in rgbImg[x:x+div,y:y+div]:\n",
    "                                continue\n",
    "                            tm = t\n",
    "                            if t == 10000:\n",
    "                                break\n",
    "                            grgr = cv2.resize(rgbImg[x:x+div,y:y+div], (256, 256), interpolation = cv2.INTER_AREA)\n",
    "                            out_name = TILE_FILENAME_FORMAT.format(triplet_id=tm,tile_type='anchor')\n",
    "                            \n",
    "                            out_name = PATH_TRAIN+out_name\n",
    "                            cv2.imwrite(out_name, grgr)\n",
    "                            t += 1\n",
    "                            labs.append(idx)\n",
    "                    \n",
    "            if t == 10000:\n",
    "                break\n",
    "    with open(DATASETS_path+\"MULTI_SCALE/\"+str(div)+\"/labels.pickle\", 'wb') as handle:\n",
    "        pickle.dump(labs, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b98c09f623885d5b0bd80e3dec26763da8c2dc37efb14e056fac55c82470d720"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
